---
title: Karpathy Attention Tweet
---

[Karpathy Attention Tweet](https://x.com/karpathy/status/1864023344435380613?lang=en)

## 개요
Andrej Karpathy가 트랜스포머의 핵심 연산자인 '어텐션'의 기원에 대한 Dzmitry Bahdanau와의 이메일 내용을 공유한 트윗입니다. Karpathy는 어텐션이 2017년 "Attention is All You Need" 논문보다 3년 앞선 2014년 Bahdanau 등의 "Neural Machine Translation by Jointly Learning to Align and Translate" 논문에서 처음 소개되었음을 강조합니다. 이메일 내용은 '어텐션'이라는 이름이 어떻게 붙게 되었는지, 당시 비슷한 아이디어들이 어떻게 논의되었는지 등의 개발 비화를 담고 있습니다.

## 핵심 키워드
*   어텐션 (Attention)
*   트랜스포머 (Transformer)
*   Andrej Karpathy
*   Dzmitry Bahdanau
*   Attention is All You Need
*   Neural Machine Translation
*   가중 평균 (weighted average)
*   RNNSearch
*   Alex Graves
*   Jason Weston

## 시사점
*   획기적인 아이디어("Attention")가 처음 등장한 논문보다, 그 아이디어를 핵심으로 삼아 다른 요소들을 성공적으로 결합하고 단순화한 논문("Attention is All You Need")이 더 큰 주목을 받을 수 있음을 보여줍니다.
*   '어텐션'이라는 용어가 인간의 인지 과정(번역 시 원문 단어에 '주의'를 기울이는 것)에서 영감을 받아 이름 붙여졌다는 점을 알 수 있습니다.
*   하나의 혁신이 여러 연구자들의 동시대적 고민과 아이디어 속에서 탄생하는 과학적 진보의 본질을 엿볼 수 있습니다.
