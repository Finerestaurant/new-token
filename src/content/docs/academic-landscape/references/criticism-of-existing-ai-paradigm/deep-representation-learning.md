---
title: Deep Representation Learning
---

[Learning Deep Representations of Data Distributions](https://ma-lab-berkeley.github.io/deep-representation-learning-book/)

## 개요

이 장은 데이터 분포의 심층 표현 학습에 대한 비공식적인 소개를 제공한다. 지능의 진화 과정을 계통 발생적(phylogenetic), 개체 발생적(ontogenetic), 사회적(societal), 과학적(scientific) 지능으로 구분하여 설명하며, 생명체가 예측 가능한 환경에 적응하고 학습하는 방식을 조명한다. 또한, 1940년대 노버트 위너(Norbert Wiener)의 사이버네틱스 운동과 1950년대 인공지능(AI)의 등장을 통해 기계 지능의 기원을 탐구한다. 핵심 주제는 자연적이든 인공적이든 지능이 방대한 데이터에서 예측 가능한 정보를 학습하는 데 의존한다는 점이다. 데이터의 본질적인 특성인 "예측 가능성"과 "저차원성" 개념을 소개하며, 유용한 데이터가 고차원 공간 내의 저차원 표면에 존재한다는 점을 강조한다. 최근의 "AI 르네상스"는 인식, 생성, 예측과 같은 동물 수준 지능의 특성을 모방하는 데 중점을 두므로, 사이버네틱스의 목표에 더 가깝다고 주장한다.

## 핵심 키워드

*   **지능 (Intelligence)**: 계통 발생적, 개체 발생적, 사회적, 과학적 지능의 진화 단계.
*   **사이버네틱스 (Cybernetics)**: 노버트 위너가 주창한 동물과 기계의 제어 및 통신에 관한 연구로, 피드백 메커니즘을 통한 지능 이해.
*   **인공지능 (Artificial Intelligence)**: 다트머스 워크숍에서 제안된 인간 수준의 지능 모방 목표, 추상화, 상징적 방법론 강조.
*   **예측 가능성 (Predictability)**: 계산 가능한 시퀀스, 자기회귀(autoregression), 연속 프로세스를 통해 데이터의 미래 값을 예측하는 능력.
*   **저차원성 (Low-Dimensionality)**: 고차원 공간 내에서 데이터가 본질적으로 낮은 차원의 표면(submanifold)에 존재한다는 특성. 주성분 분석(PCA)과 관련되며, 데이터 완성(completion), 노이즈 제거(denoising), 오류 수정(error correction) 등의 이점을 제공.
*   **심층 표현 (Deep Representations)**: 비선형성 처리 및 지능 시스템의 다양한 메커니즘 통합을 가능하게 하는 핵심 개념.
*   **기계 지능 (Machine Intelligence)**: 현대 심층 신경망을 기반으로 한 인식, 생성, 예측, 강화 학습 등의 기술.

## 시사점

이 장은 자연 및 인공 지능을 이해하기 위해 예측 가능한 정보가 어떻게 모델링되고, 학습되며, 조직되는지에 대한 과학적이고 수학적인 접근 방식의 중요성을 강조한다. 예측 가능성과 저차원성 개념은 모든 현대 학습 방법의 근간을 이루며, 암묵적이든 명시적이든 활용된다. 이 책은 심층 네트워크가 비선형성을 효과적으로 처리하고 지능 시스템의 다양한 메커니즘을 통합하는 방법을 보여줌으로써 이론과 실제를 연결하는 것을 목표로 한다. 현재의 "AI 르네상스"는 주로 인식 및 예측과 같은 동물 수준의 지능을 모방하므로, "사이버네틱스 르네상스"로 부르는 것이 더 적절하다고 제안한다. 저차원성의 특성(데이터 완성, 노이즈 제거, 오류 수정 등)은 실제 데이터에서 이러한 구조를 학습하는 실질적인 이점을 부각한다. 궁극적으로 이 책은 지능에 대한 근본적인 질문에 답하고, 그 계산 원리와 메커니즘을 이해하며, 현재의 성과와 남은 과제를 명확히 하여 진정한 지능을 향한 미래 연구 방향을 제시하는 데 기여하고자 한다.