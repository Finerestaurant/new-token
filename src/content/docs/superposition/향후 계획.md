# Superposition.md 문서 재작성 계획

## 개요

현재 superposition.md 문서의 내용이 직관적으로 이해하기 어렵다는 문제가 있습니다. 이 문서를 독자 친화적이고 이해하기 쉬운 형태로 재작성하기 위한 계획을 정리합니다.

## 재작성의 목표

1. **논문을 읽는 방법 제시**: 독자가 "Toy Models of Superposition" 논문을 어떻게 접근해야 하는지 안내
2. **논문의 의의 설명**: 이 연구가 가지는 학문적, 실용적 의의를 명확히 전달
3. **숨겨진 전제 드러내기**: 논문 저자들이 명시적으로 설명하지 않은 전제들을 밝혀내고 설명

## 문제 인식

### 핵심 문제점

"feature가 superposition되어있다"는 주장을 처음 접했을 때:
- **어떤 전제** 위에서 그러한 주장을 하는지가 불명확함
- 그 **토대**를 모른 채로 논문을 받아들이면 이해가 어려움
- 논문의 저자들이 그 전제를 친절하게 설명하지 않음

#### 마르코프 체인의 예시: 수식만으로는 충분하지 않다

이 문제를 이해하기 위해 마르코프 체인(Markov Chain)을 예로 들어보겠습니다.

마르코프 체인을 수식만 가지고 이해하려고 하면, 그것은 근의 공식과 다를 바가 없는 수식에 불과합니다. 하지만:

- **왜 마르코프(Andrey Markov)는 이 수식을 만들었는가?**
  - 1906년, 마르코프는 독립적이지 않은 사건들의 연쇄를 모델링하기 위해 이 개념을 개발했습니다
  - 당시 러시아 문학 작품에서 자음과 모음의 패턴을 분석하는 데 적용했습니다
  
- **그 시대의 배경은 어땠는가?**
  - 확률론이 독립 사건 중심으로 발전하던 시기에, 의존적 사건의 수학적 모델이 필요했습니다
  
- **이 수식으로 무엇을 표현하고 싶었는가?**
  - "현재 상태만 알면 미래를 예측할 수 있다"는 메모리 없는(memoryless) 특성을 수학적으로 표현
  - P(X_{n+1} | X_n, X_{n-1}, ..., X_1) = P(X_{n+1} | X_n)
  
- **나중에 어떻게 활용되었는가?**
  - 강화학습(Reinforcement Learning)에서 Markov Decision Process(MDP)로 확장
  - Value Function의 개념이 붙으면서 V(s) = E[R_{t+1} + γV(s_{t+1}) | s_t = s]로 발전
  - 현재는 음성 인식, 자연어 처리, 금융 모델링 등 다양한 분야에 활용

**이처럼 수식의 역사적 맥락과 저자의 의도를 이해하지 못하면, 그 본질을 파악하기 어렵습니다.**

#### Transformer Circuits 연구의 흐름을 쫓아가기

이 논문도 마찬가지입니다. 저자들은 [Transformer Circuits](https://transformer-circuits.pub/)라는 사이트에서 여러 XAI(eXplainable AI) 연구를 진행해온 흐름이 있었습니다:

1. **[Zoom In: An Introduction to Circuits (2020)](https://distill.pub/2020/circuits/zoom-in/)** - 신경망의 개별 뉴런과 회로를 시각화하는 기초 연구
2. **[A Mathematical Framework for Transformer Circuits (2021)](https://transformer-circuits.pub/2021/framework/index.html)** - Transformer의 내부 메커니즘을 수학적으로 분석
3. **[In-context Learning and Induction Heads (2022)](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)** - 문맥 학습의 메커니즘 발견
4. **[Toy Models of Superposition (2022)](https://transformer-circuits.pub/2022/toy_model/index.html)** - 본 논문, 중첩 현상의 기초 이론 구축
5. **[Towards Monosemanticity (2023)](https://transformer-circuits.pub/2023/monosemantic-features/index.html)** - Sparse Autoencoder를 통한 특징 분리
6. **[Scaling Monosemanticity (2024)](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)** - 대규모 모델에서의 특징 해석

**이 논문은 고립된 연구가 아니라, Mechanistic Interpretability라는 거대한 연구 흐름의 한 부분입니다.** 그 근본을 쫓아가는 작업이 필요하며, 이후 섹션에서 이 논문들을 함께 살펴볼 예정입니다.

### Feature의 개념적 혼란

#### 세간에서 사용되는 "Feature"의 의미

일반적으로 머신러닝 커뮤니티에서 "feature"는 다음과 같이 사용됩니다:

1. **입력 변수로서의 Feature** (Scikit-learn 문서)
   > "Features are individual measurable properties of the phenomena being observed... In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon."
   > 
   > — *참고: [Scikit-learn Glossary](https://scikit-learn.org/stable/glossary.html#term-feature)*

2. **데이터 테이블의 열(Column)** (Kaggle 등 데이터 과학 실무)
   > "A feature is a column in a dataset. Each feature represents a specific attribute or characteristic of the data."
   
3. **입력 벡터의 차원**
   > "In the context of neural networks, features typically refer to the input dimensions or the individual elements of the input vector."

**핵심: 일반적으로 feature는 "데이터의 속성", "측정 가능한 변수", "입력의 차원"으로 이해됩니다. 물리적 형태나 기하학적 구조를 가진 객체로 보지 않습니다.**

#### 논문에서 사용되는 "Feature"의 의미

반면, Toy Models of Superposition 논문에서 feature는 완전히 다른 방식으로 정의됩니다:

> **"We'll define features as properties of the input which a sufficiently large neural network would dedicate a neuron to representing."**
> 
> — *[Toy Models of Superposition, Section "What are Features?"](https://transformer-circuits.pub/2022/toy_model/index.html#motivation-features)*

그리고 더 나아가:

> **"Features are directions in activation space... Features can be thought of as directions in activation space that the network can read from and write to."**
> 
> — *[Toy Models of Superposition, Section "Definitions and Motivation"](https://transformer-circuits.pub/2022/toy_model/index.html#setup-intuition)*

**핵심: 논문에서 feature는 "활성화 공간의 방향(direction)"이며, "물리적으로 조작 가능한 벡터"입니다.**

#### 개념적 간극

- **일반적 이해**: Feature = 데이터의 속성 (추상적 개념)
- **논문의 정의**: Feature = 활성화 공간의 벡터 방향 (물리적 객체)

이 간극이 바로 논문을 처음 읽을 때 "feature가 중첩된다"는 주장이 낯설게 느껴지는 이유입니다. 

추상적 개념(나이, 성별, 가격)이 어떻게 "중첩"될 수 있는가? 하지만 만약 feature를 "고차원 공간의 벡터 방향"으로 본다면, 이 벡터들이 서로 직교하지 않고 **비스듬하게 배치되어 겹쳐있다**는 의미로 "중첩"을 이해할 수 있습니다.

## 재작성 방향

### 1. 스레드 형식의 구성

- 두서에 눈에 확 들어오는 **질문을 먼저** 제시
- 독자의 호기심을 자극하며 글을 이어나가는 방식

### 2. 오프닝 문장 (초안)

```
여러분은 이 논문을 이해하셨나요? 

"feature가 중첩된다"... feature가 뭐고, 중첩인 superposition은 뭐지? 

딥러닝을 공부하신 분은 아시겠지만, feature라는 단어는 주로 머신러닝에서 
개별 변수들을 의미할 때 사용하거나, 진짜 의미적으로 "특징"을 의미할 때 
사용하지, feature라는 것이 물리적으로, 혹은 기하학적으로 어떤 형태를 
지녀야 한다라는 개념은 흔치 않을 겁니다. 

근데, 이 논문에서는 feature라는 말을 물리적으로 조작 가능한 형태의 
구체적인 모습을 갖추었고, 그 특징으로 인해 중첩된다고 이야기하고 있어요.

(자막: feature가 중첩된다)

그럼, 이 사람들이 저 주장을 하기 위해 feature를 어떻게 정의했는지 
살펴보도록 하겠습니다.
```

### 3. "What are Features?" 섹션 연결

- 논문의 "What are Features?" 섹션을 언급
- 저자들도 feature를 정의하기가 어려웠다는 점을 강조
- 원래 정의하기 어려운 개념이라는 것을 인정
- 주장을 이어나가기 위해서 **느슨하게 정의**했다는 느낌을 전달

### 4. 전개 방식

1. **문제 제기**: 논문을 읽으면서 생기는 자연스러운 의문들을 나열
2. **전제 탐색**: 논문이 가정하는 숨겨진 전제들을 하나씩 드러냄
3. **개념 정의**: Feature, superposition 등 핵심 개념을 독자의 관점에서 재정의
4. **논문 분석**: 논문의 주장을 전제를 이해한 상태에서 다시 살펴봄
5. **의의 도출**: 이 연구가 왜 중요한지, 무엇을 시사하는지 설명

## 구성 요소

### 필수 포함 내용

1. ✅ 독자 친화적인 질문으로 시작
2. ✅ Feature 개념의 일반적 이해 vs 논문에서의 정의 대조
3. ✅ "What are Features?" 섹션 언급
4. ✅ 저자들의 정의가 느슨함을 인정
5. ⬜ Superposition이 왜 발생하는지에 대한 직관적 설명
6. ⬜ 실제 예시를 통한 이해 촉진
7. ⬜ Multi-Stream 아키텍처와의 연결성

## 작성 원칙

- **친절함**: 독자가 모르는 것이 당연하다는 태도
- **대화체**: 강의하듯이, 대화하듯이 설명
- **단계적**: 한 번에 하나의 개념씩 차근차근
- **질문 중심**: "왜?", "어떻게?"를 먼저 던지고 답하기
- **솔직함**: 어려운 것은 어렵다고, 모호한 것은 모호하다고 인정

## 다음 단계

1. 위의 오프닝 문장을 바탕으로 전체 초안 작성
2. "What are Features?" 섹션의 내용을 분석하고 요약
3. 숨겨진 전제들을 목록화
4. 각 전제에 대한 설명 작성
5. 전체 흐름 검토 및 수정
