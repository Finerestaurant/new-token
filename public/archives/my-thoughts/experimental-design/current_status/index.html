<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/new-token/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=new-token/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>기존 LLM의 구조 및 토큰 학습 방식 분석 | The Polymath&#39;s Log</title>
<meta name="keywords" content="">
<meta name="description" content="기존 대규모 언어 모델(LLM)의 기본 구조와 토큰 학습 메커니즘을 분석합니다.">
<meta name="author" content="Anthony Park">
<link rel="canonical" href="http://localhost:1313/new-token/archives/my-thoughts/experimental-design/current_status/">
<link crossorigin="anonymous" href="/new-token/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css" integrity="sha256-2jIR5e&#43;Ge/K3X9WmUVz&#43;1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/new-token/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/new-token/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/new-token/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/new-token/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/new-token/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/new-token/archives/my-thoughts/experimental-design/current_status/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="http://localhost:1313/new-token/archives/my-thoughts/experimental-design/current_status/">
  <meta property="og:site_name" content="The Polymath&#39;s Log">
  <meta property="og:title" content="기존 LLM의 구조 및 토큰 학습 방식 분석">
  <meta property="og:description" content="기존 대규모 언어 모델(LLM)의 기본 구조와 토큰 학습 메커니즘을 분석합니다.">
  <meta property="og:locale" content="ko-kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="archives">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="기존 LLM의 구조 및 토큰 학습 방식 분석">
<meta name="twitter:description" content="기존 대규모 언어 모델(LLM)의 기본 구조와 토큰 학습 메커니즘을 분석합니다.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Archives",
      "item": "http://localhost:1313/new-token/archives/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "기존 LLM의 구조 및 토큰 학습 방식 분석",
      "item": "http://localhost:1313/new-token/archives/my-thoughts/experimental-design/current_status/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "기존 LLM의 구조 및 토큰 학습 방식 분석",
  "name": "기존 LLM의 구조 및 토큰 학습 방식 분석",
  "description": "기존 대규모 언어 모델(LLM)의 기본 구조와 토큰 학습 메커니즘을 분석합니다.",
  "keywords": [
    
  ],
  "articleBody": "최신 대규모 언어 모델(LLM)은 주로 트랜스포머(Transformer) 아키텍처를 기반으로 합니다. 이 아키텍처는 인코더-디코더 또는 디코더 전용 스택으로 구성될 수 있으며, 셀프-어텐션(Self-Attention) 메커니즘을 핵심으로 활용하여 입력 시퀀스 내의 다양한 위치에 있는 토큰 간의 관계를 효율적으로 모델링합니다.\n1. LLM의 대략적인 구조 임베딩 계층 (Embedding Layer):\n입력 텍스트는 먼저 개별 토큰(단어, 단어 조각, 또는 문자)으로 분리됩니다. 각 토큰은 고차원 벡터 공간의 “임베딩\"으로 변환됩니다. 이 임베딩은 토큰의 의미론적 및 구문론적 정보를 포함합니다. 위치 임베딩(Positional Embedding)이 추가되어 시퀀스 내 토큰의 순서 정보를 모델에 제공합니다. 트랜스포머 블록 (Transformer Blocks):\n여러 개의 동일한 트랜스포머 블록이 쌓여 LLM의 핵심을 이룹니다. 각 블록은 주로 두 가지 하위 계층으로 구성됩니다: 멀티헤드 셀프-어텐션 (Multi-Head Self-Attention): 모델이 입력 시퀀스의 다른 부분에 “집중\"하여 각 토큰에 대한 표현을 계산할 수 있게 합니다. 여러 “헤드\"는 다양한 관점에서 관계를 학습합니다. 피드포워드 네트워크 (Feed-Forward Network): 어텐션 계층의 출력을 비선형적으로 변환하여 모델의 표현력을 높입니다. 각 하위 계층 후에는 잔차 연결(Residual Connection)과 계층 정규화(Layer Normalization)가 적용되어 학습을 안정화하고 깊은 모델의 훈련을 용이하게 합니다. 출력 계층 (Output Layer):\n트랜스포머 스택의 최종 출력은 선형 계층과 소프트맥스(Softmax) 함수를 통해 다음 토큰에 대한 확률 분포로 변환됩니다. 모델은 이 확률 분포를 기반으로 다음으로 올 토큰을 예측합니다. 2. LLM의 토큰 학습 능력 LLM의 “토큰 학습\"은 크게 두 가지 관점에서 이해할 수 있습니다:\n가. 사전 학습 (Pre-training) 단계에서의 토큰 학습 어휘 집합 (Vocabulary): LLM은 방대한 텍스트 코퍼스를 기반으로 고정된 어휘 집합(Vocabulary)을 구축합니다. 이 어휘 집합은 모델이 인코딩하고 디코딩할 수 있는 모든 고유한 토큰을 포함합니다. 일반적으로 WordPiece, SentencePiece, BPE(Byte Pair Encoding)와 같은 서브워드 토크나이저를 사용하여 희귀 단어나 새로운 단어도 효율적으로 처리할 수 있도록 합니다. 토큰 임베딩 학습: 사전 학습 과정에서 각 토큰에 대한 임베딩 벡터가 학습됩니다. 유사한 의미를 가진 토큰은 벡터 공간에서 가깝게 위치하도록 학습되어, 모델이 단어의 의미론적 관계를 파악할 수 있게 합니다. 문맥적 표현 학습: 트랜스포머의 셀프-어텐션 메커니즘을 통해 모델은 주어진 문맥 내에서 각 토큰의 의미를 동적으로 파악하고, 이를 기반으로 풍부한 문맥적 표현(contextualized embeddings)을 생성합니다. 이는 단순히 고정된 임베딩을 사용하는 것을 넘어, 단어의 다의성을 처리하고 복잡한 언어 현상을 이해하는 데 필수적입니다. 나. 새로운 토큰(개념)에 대한 적응 LLM은 기본적으로 사전 학습된 어휘 집합 내의 토큰을 사용하여 작동합니다. 하지만, “새로운 토큰을 배우는” 능력은 다음과 같은 방식으로 발현될 수 있습니다:\n미세 조정 (Fine-tuning): 특정 도메인이나 태스크에 맞춰 모델을 미세 조정할 때, 모델은 기존 어휘 집합 내의 토큰들로 구성된 새로운 패턴과 관계를 학습합니다. 예를 들어, 의학 도메인의 새로운 용어가 등장하더라도, 해당 용어가 기존 토큰의 조합(서브워드)으로 표현될 수 있다면, 모델은 미세 조정을 통해 이 용어와 관련된 새로운 문맥적 의미를 학습할 수 있습니다. 퓨샷 학습 (Few-shot Learning) 및 인컨텍스트 학습 (In-context Learning): 모델이 명시적으로 재훈련되지 않고도, 프롬프트 내에 제공된 몇 가지 예시를 통해 새로운 개념이나 패턴을 ‘습득’하는 능력입니다. 이는 모델이 기존에 학습한 지식과 일반화 능력을 활용하여 새로운 정보를 추론하고 적용하는 방식입니다. 여기서 “새로운 토큰\"이라기보다는 “새로운 개념” 또는 “새로운 용법\"에 대한 적응에 가깝습니다. 어휘 확장 (Vocabulary Extension): 드물지만, 완전히 새로운 토큰을 모델의 어휘 집합에 추가하고 이에 대한 임베딩을 학습시키는 연구도 진행되고 있습니다. 이는 모델의 구조를 직접 수정하고 추가 학습을 필요로 하는 더 복잡한 과정입니다. 하지만 대부분의 경우, 서브워드 토크나이저가 새로운 단어를 기존 토큰의 조합으로 분해하여 처리하므로, 모델이 완전히 새로운 원자적 토큰을 ‘발명’할 필요는 적습니다. 실험설계의 목표는 LLM이 이러한 기본적인 토큰 처리 및 학습 메커니즘을 넘어, 마치 인간이 새로운 단어를 배우고 사용하는 것처럼 ‘새로운 토큰’ 혹은 ‘새로운 개념 단위’를 능동적으로 생성하고 내부적으로 표현하며, 이를 통해 유연하게 지식을 확장하는 모습을 보이는 것입니다. 이는 단순히 기존 어휘의 조합으로 새로운 의미를 표현하는 것을 넘어서는, 모델의 근본적인 적응 및 생성 능력을 탐구하는 방향이 될 것입니다.\n",
  "wordCount" : "546",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Anthony Park"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/new-token/archives/my-thoughts/experimental-design/current_status/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "The Polymath's Log",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/new-token/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/new-token/" accesskey="h" title="The Polymath&#39;s Log (Alt + H)">The Polymath&#39;s Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/new-token/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/new-token/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/new-token/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/new-token/contact/" title="Contact">
                    <span>Contact</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      기존 LLM의 구조 및 토큰 학습 방식 분석
    </h1>
    <div class="post-description">
      기존 대규모 언어 모델(LLM)의 기본 구조와 토큰 학습 메커니즘을 분석합니다.
    </div>
    <div class="post-meta"><span>Anthony Park</span>

</div>
  </header> 
  <div class="post-content"><p>최신 대규모 언어 모델(LLM)은 주로 트랜스포머(Transformer) 아키텍처를 기반으로 합니다. 이 아키텍처는 인코더-디코더 또는 디코더 전용 스택으로 구성될 수 있으며, 셀프-어텐션(Self-Attention) 메커니즘을 핵심으로 활용하여 입력 시퀀스 내의 다양한 위치에 있는 토큰 간의 관계를 효율적으로 모델링합니다.</p>
<h2 id="1-llm의-대략적인-구조">1. LLM의 대략적인 구조<a hidden class="anchor" aria-hidden="true" href="#1-llm의-대략적인-구조">#</a></h2>
<ul>
<li>
<p><strong>임베딩 계층 (Embedding Layer)</strong>:</p>
<ul>
<li>입력 텍스트는 먼저 개별 토큰(단어, 단어 조각, 또는 문자)으로 분리됩니다.</li>
<li>각 토큰은 고차원 벡터 공간의 &ldquo;임베딩&quot;으로 변환됩니다. 이 임베딩은 토큰의 의미론적 및 구문론적 정보를 포함합니다.</li>
<li>위치 임베딩(Positional Embedding)이 추가되어 시퀀스 내 토큰의 순서 정보를 모델에 제공합니다.</li>
</ul>
</li>
<li>
<p><strong>트랜스포머 블록 (Transformer Blocks)</strong>:</p>
<ul>
<li>여러 개의 동일한 트랜스포머 블록이 쌓여 LLM의 핵심을 이룹니다.</li>
<li>각 블록은 주로 두 가지 하위 계층으로 구성됩니다:
<ul>
<li><strong>멀티헤드 셀프-어텐션 (Multi-Head Self-Attention)</strong>: 모델이 입력 시퀀스의 다른 부분에 &ldquo;집중&quot;하여 각 토큰에 대한 표현을 계산할 수 있게 합니다. 여러 &ldquo;헤드&quot;는 다양한 관점에서 관계를 학습합니다.</li>
<li><strong>피드포워드 네트워크 (Feed-Forward Network)</strong>: 어텐션 계층의 출력을 비선형적으로 변환하여 모델의 표현력을 높입니다.</li>
</ul>
</li>
<li>각 하위 계층 후에는 잔차 연결(Residual Connection)과 계층 정규화(Layer Normalization)가 적용되어 학습을 안정화하고 깊은 모델의 훈련을 용이하게 합니다.</li>
</ul>
</li>
<li>
<p><strong>출력 계층 (Output Layer)</strong>:</p>
<ul>
<li>트랜스포머 스택의 최종 출력은 선형 계층과 소프트맥스(Softmax) 함수를 통해 다음 토큰에 대한 확률 분포로 변환됩니다.</li>
<li>모델은 이 확률 분포를 기반으로 다음으로 올 토큰을 예측합니다.</li>
</ul>
</li>
</ul>
<h2 id="2-llm의-토큰-학습-능력">2. LLM의 토큰 학습 능력<a hidden class="anchor" aria-hidden="true" href="#2-llm의-토큰-학습-능력">#</a></h2>
<p>LLM의 &ldquo;토큰 학습&quot;은 크게 두 가지 관점에서 이해할 수 있습니다:</p>
<h3 id="가-사전-학습-pre-training-단계에서의-토큰-학습">가. 사전 학습 (Pre-training) 단계에서의 토큰 학습<a hidden class="anchor" aria-hidden="true" href="#가-사전-학습-pre-training-단계에서의-토큰-학습">#</a></h3>
<ul>
<li><strong>어휘 집합 (Vocabulary)</strong>: LLM은 방대한 텍스트 코퍼스를 기반으로 고정된 어휘 집합(Vocabulary)을 구축합니다. 이 어휘 집합은 모델이 인코딩하고 디코딩할 수 있는 모든 고유한 토큰을 포함합니다. 일반적으로 WordPiece, SentencePiece, BPE(Byte Pair Encoding)와 같은 서브워드 토크나이저를 사용하여 희귀 단어나 새로운 단어도 효율적으로 처리할 수 있도록 합니다.</li>
<li><strong>토큰 임베딩 학습</strong>: 사전 학습 과정에서 각 토큰에 대한 임베딩 벡터가 학습됩니다. 유사한 의미를 가진 토큰은 벡터 공간에서 가깝게 위치하도록 학습되어, 모델이 단어의 의미론적 관계를 파악할 수 있게 합니다.</li>
<li><strong>문맥적 표현 학습</strong>: 트랜스포머의 셀프-어텐션 메커니즘을 통해 모델은 주어진 문맥 내에서 각 토큰의 의미를 동적으로 파악하고, 이를 기반으로 풍부한 문맥적 표현(contextualized embeddings)을 생성합니다. 이는 단순히 고정된 임베딩을 사용하는 것을 넘어, 단어의 다의성을 처리하고 복잡한 언어 현상을 이해하는 데 필수적입니다.</li>
</ul>
<h3 id="나-새로운-토큰개념에-대한-적응">나. 새로운 토큰(개념)에 대한 적응<a hidden class="anchor" aria-hidden="true" href="#나-새로운-토큰개념에-대한-적응">#</a></h3>
<p>LLM은 기본적으로 사전 학습된 어휘 집합 내의 토큰을 사용하여 작동합니다. 하지만, &ldquo;새로운 토큰을 배우는&rdquo; 능력은 다음과 같은 방식으로 발현될 수 있습니다:</p>
<ul>
<li><strong>미세 조정 (Fine-tuning)</strong>: 특정 도메인이나 태스크에 맞춰 모델을 미세 조정할 때, 모델은 기존 어휘 집합 내의 토큰들로 구성된 새로운 패턴과 관계를 학습합니다. 예를 들어, 의학 도메인의 새로운 용어가 등장하더라도, 해당 용어가 기존 토큰의 조합(서브워드)으로 표현될 수 있다면, 모델은 미세 조정을 통해 이 용어와 관련된 새로운 문맥적 의미를 학습할 수 있습니다.</li>
<li><strong>퓨샷 학습 (Few-shot Learning) 및 인컨텍스트 학습 (In-context Learning)</strong>: 모델이 명시적으로 재훈련되지 않고도, 프롬프트 내에 제공된 몇 가지 예시를 통해 새로운 개념이나 패턴을 &lsquo;습득&rsquo;하는 능력입니다. 이는 모델이 기존에 학습한 지식과 일반화 능력을 활용하여 새로운 정보를 추론하고 적용하는 방식입니다. 여기서 &ldquo;새로운 토큰&quot;이라기보다는 &ldquo;새로운 개념&rdquo; 또는 &ldquo;새로운 용법&quot;에 대한 적응에 가깝습니다.</li>
<li><strong>어휘 확장 (Vocabulary Extension)</strong>: 드물지만, 완전히 새로운 토큰을 모델의 어휘 집합에 추가하고 이에 대한 임베딩을 학습시키는 연구도 진행되고 있습니다. 이는 모델의 구조를 직접 수정하고 추가 학습을 필요로 하는 더 복잡한 과정입니다. 하지만 대부분의 경우, 서브워드 토크나이저가 새로운 단어를 기존 토큰의 조합으로 분해하여 처리하므로, 모델이 완전히 새로운 원자적 토큰을 &lsquo;발명&rsquo;할 필요는 적습니다.</li>
</ul>
<p>실험설계의 목표는 LLM이 이러한 기본적인 토큰 처리 및 학습 메커니즘을 넘어, 마치 인간이 새로운 단어를 배우고 사용하는 것처럼 &lsquo;새로운 토큰&rsquo; 혹은 &lsquo;새로운 개념 단위&rsquo;를 능동적으로 생성하고 내부적으로 표현하며, 이를 통해 유연하게 지식을 확장하는 모습을 보이는 것입니다. 이는 단순히 기존 어휘의 조합으로 새로운 의미를 표현하는 것을 넘어서는, 모델의 근본적인 적응 및 생성 능력을 탐구하는 방향이 될 것입니다.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/new-token/">The Polymath&#39;s Log</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
