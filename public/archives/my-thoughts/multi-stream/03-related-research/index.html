<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Multi-Stream: 관련 연구 심층 분석 | The Polymath&#39;s Log</title>
<meta name="keywords" content="">
<meta name="description" content="Multi-Stream 아키텍처와 관련된 주요 연구(Shanks, LSLM, PersonaPlex 등)들을 논문 단위로 심층 분석합니다.">
<meta name="author" content="Anthony Park">
<link rel="canonical" href="https://Finerestaurant.github.io/new-token/archives/my-thoughts/multi-stream/03-related-research/">
<link crossorigin="anonymous" href="/new-token/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css" integrity="sha256-2jIR5e&#43;Ge/K3X9WmUVz&#43;1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://Finerestaurant.github.io/new-token/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://Finerestaurant.github.io/new-token/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Finerestaurant.github.io/new-token/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://Finerestaurant.github.io/new-token/apple-touch-icon.png">
<link rel="mask-icon" href="https://Finerestaurant.github.io/new-token/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://Finerestaurant.github.io/new-token/archives/my-thoughts/multi-stream/03-related-research/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://Finerestaurant.github.io/new-token/archives/my-thoughts/multi-stream/03-related-research/">
  <meta property="og:site_name" content="The Polymath&#39;s Log">
  <meta property="og:title" content="Multi-Stream: 관련 연구 심층 분석">
  <meta property="og:description" content="Multi-Stream 아키텍처와 관련된 주요 연구(Shanks, LSLM, PersonaPlex 등)들을 논문 단위로 심층 분석합니다.">
  <meta property="og:locale" content="ko-kr">
  <meta property="og:type" content="article">
    <meta property="article:section" content="archives">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-Stream: 관련 연구 심층 분석">
<meta name="twitter:description" content="Multi-Stream 아키텍처와 관련된 주요 연구(Shanks, LSLM, PersonaPlex 등)들을 논문 단위로 심층 분석합니다.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Archives",
      "item": "https://Finerestaurant.github.io/new-token/archives/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Multi-Stream: 관련 연구 심층 분석",
      "item": "https://Finerestaurant.github.io/new-token/archives/my-thoughts/multi-stream/03-related-research/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Multi-Stream: 관련 연구 심층 분석",
  "name": "Multi-Stream: 관련 연구 심층 분석",
  "description": "Multi-Stream 아키텍처와 관련된 주요 연구(Shanks, LSLM, PersonaPlex 등)들을 논문 단위로 심층 분석합니다.",
  "keywords": [
    
  ],
  "articleBody": "Multi-Stream 아키텍처와 관련된 주요 연구들을 논문 단위로, 각각 개요, 아키텍처, 벤치마크, 학습 방식으로 나누어 상세히 분석합니다.\n3.1. Shanks: Simultaneous Hearing and Thinking for Spoken Language Models 논문: “Shanks: Simultaneous Hearing and Thinking for Spoken Language Models” (arXiv 2024) 핵심 의의: 사용자가 말하는 동안 모델이 동시에 “내면의 추론(unspoken reasoning)“을 생성하는 범용 추론 프레임워크를 제안했습니다. 이는 기존 모델들이 사용자의 발화가 끝난 후에야 처리를 시작하여 응답 지연이 발생하는 한계를 극복하고, 실시간 상호작용을 구현하기 위한 중요한 접근 방식입니다. 3.1.1. 개요 (Overview) Shanks는 스트리밍되는 음성 입력을 고정된 길이의 청크(chunk) 단위로 처리합니다. 각 음성 청크를 받을 때마다, 이전에 입력된 모든 음성과 내부 추론을 바탕으로 새로운 “사고(thinking)” 청크를 생성합니다. 이 내부적인 사고 과정을 통해 모델은 사용자의 말을 끊고 개입할지(interrupt) 또는 API 같은 도구를 호출할지 등을 사용자가 아직 말하는 도중에 결정할 수 있습니다. 이 프레임워크는 End-to-End SLM과 Cascade SLM(ASR+LLM) 두 가지 방식 모두에 적용 가능하도록 설계되었습니다.\n3.1.2. 아키텍처 (Architecture) Shanks는 사용자 음성을 고정된 시간 단위의 청크(S₁, S₂, …)로 분할하여 처리합니다. i번째 음성 청크(Sᵢ)를 입력받으면, 이를 바탕으로 i번째 사고 청크(Rᵢ)를 생성합니다. 즉, 사용자가 (i+1)번째 음성 청크를 말하는 동안, 모델은 i번째 사고 청크를 내부적으로 생성하며 ‘듣는 것’과 ‘생각하는 것’을 동시에 수행합니다. 이 과정에서 생성된 사고 청크(Rᵢ)는 발화되지 않으며, 모델의 내부적인 추론으로만 사용됩니다.\nShanks-E2E: End-to-End Spoken Language Model을 파인튜닝하여 ‘들으면서 생각하는’ 능력을 학습시킵니다. Shanks-Cascade: ASR 모델과 강력한 텍스트 기반 LLM을 계단식으로 연결하여 더 강력한 추론 능력을 활용합니다. 3.1.3. 벤치마크 (Benchmarks) 사용자 발화 중단 (수학 문제 풀이): Shanks-Cascade 모델은 78.3%의 유효한 중단률을 보여 가장 뛰어난 성능을 기록했습니다. 청취 중 도구 사용 (목적 지향 대화): Shanks는 전체 API 호출의 56.9%를 사용자가 말을 마_치기 전에_ 성공적으로 수행했습니다. 3.1.4. 학습 (Training) GPT-4o를 사용하여 ‘사고 청크’와 중단/도구호출 시점을 포함한 학습 데이터를 생성하고, 표준 언어 모델링 손실 함수로 순차적으로 예측하도록 학습합니다.\n3.2. Language Model Can Listen While Speaking 논문: “Language Model Can Listen While Speaking” (arXiv 2024) 핵심 의의: 실시간 양방향 상호작용을 위해 ‘말하면서 동시에 듣는’ 능력을 갖춘 End-to-End 모델(LSLM: Listening-while-Speaking Language Model)을 제안했습니다. 완전한 양방향 통신(Full Duplex Modeling)을 구현하여 실시간 중단(interruption)을 가능하게 합니다. 3.2.1. 아키텍처 (Architecture) LSLM은 다음 토큰을 예측하기 위해 말하기 채널(이전 생성 토큰)과 듣기 채널(실시간 음성 임베딩)의 정보를 융합(fusion)합니다. 트랜스포머 각 블록에서 정보를 결합하는 중간 융합(Middle Fusion) 방식이 가장 효과적이었습니다. 사용자의 중단이 감지되면, [IRQ]라는 특수 토큰을 통해 자신의 발화를 즉시 멈출 수 있습니다.\n3.2.2. 벤치마크 (Benchmarks) ‘명령어 기반’ 및 ‘음성 기반’ 중단 시나리오에서 평가되었습니다. 중간 융합 전략을 사용한 LSLM-MF 모델이 가장 좋은 성능을 보였으며, 노이즈 환경에서도 안정적인 중단 감지 능력을 보여주었습니다.\n3.2.3. 학습 (Training) TTS, 중단, 노이즈 데이터를 혼합하여 학습합니다. 50% 확률로 노이즈와 중단 상황을 무작위로 추가하여 강건성(robustness)을 높입니다.\n3.3. PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models 논문: “PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models” (NVIDIA Research 2026) 핵심 의의: 텍스트 프롬프트로 역할을 자유롭게 정의하고 다양한 음성으로 자연스러운 양방향 대화를 가능하게 하는 시스템입니다. ‘동시에 듣고 말하는’ 이중 스트림(dual-stream) 구성을 통해 자연스러운 대화 리듬을 유지합니다. 3.3.1. 아키텍처 (Architecture) Moshi 아키텍처(7B) 기반으로, 목소리 톤을 제어하는 음성 프롬프트와 역할/맥락을 정의하는 텍스트 프롬프트를 함께 사용하는 ‘하이브리드 프롬프팅’을 사용합니다. 이 이중 스트림 구성이 양방향 대화의 핵심입니다.\n3.3.2. 학습 (Training) 자연스러운 대화 데이터 부족 문제를 해결하기 위해, 실제 대화 데이터(Fisher corpus)와 합성 데이터를 혼합하여 사용합니다. LLM으로 실제 대화에 페르소나 설명을 생성하고, 합성 데이터는 스크립트와 음성을 모두 생성하여 사용합니다.\n3.4. 기타 관련 연구 Front-Loading Reasoning (NVIDIA, 2025): LLM 학습 초반(사전-학습)에 추론 데이터를 포함하는 “Front-Loading\"이 훨씬 효과적임을 증명했습니다. 사전-학습은 데이터의 다양성에, 후-학습(SFT)은 데이터의 품질에 더 민감하다는 “비대칭 데이터 원칙\"을 제시했습니다. 이는 우리 아키텍처의 학습 전략 수립에 중요한 시사점을 줍니다. Multi-Token Attention (2025): 기존 어텐션이 단일 토큰 벡터에 의존하는 병목을 해결하기 위해, 컨볼루션(convolution) 연산으로 여러 토큰을 동시에 고려하는 **Multi-Token Attention (MTA)**을 제안했습니다. 이는 모델이 더 풍부한 컨텍스트 정보를 활용하도록 하여, 우리 아키텍처의 상위 계층(추상적 예측 모델) 성능을 높이는 데 기여할 수 있습니다. ",
  "wordCount" : "597",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Anthony Park"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Finerestaurant.github.io/new-token/archives/my-thoughts/multi-stream/03-related-research/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "The Polymath's Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Finerestaurant.github.io/new-token/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Finerestaurant.github.io/new-token/" accesskey="h" title="The Polymath&#39;s Log (Alt + H)">The Polymath&#39;s Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Finerestaurant.github.io/new-token/posts/" title="글 모음 (Posts)">
                    <span>글 모음 (Posts)</span>
                </a>
            </li>
            <li>
                <a href="https://Finerestaurant.github.io/new-token/archives/" title="과거 연구 노트 (Archives)">
                    <span>과거 연구 노트 (Archives)</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Multi-Stream: 관련 연구 심층 분석
    </h1>
    <div class="post-description">
      Multi-Stream 아키텍처와 관련된 주요 연구(Shanks, LSLM, PersonaPlex 등)들을 논문 단위로 심층 분석합니다.
    </div>
    <div class="post-meta"><span>3 min</span>&nbsp;·&nbsp;<span>Anthony Park</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#31-shanks-simultaneous-hearing-and-thinking-for-spoken-language-models" aria-label="3.1. Shanks: Simultaneous Hearing and Thinking for Spoken Language Models">3.1. Shanks: Simultaneous Hearing and Thinking for Spoken Language Models</a><ul>
                        
                <li>
                    <a href="#311-%ea%b0%9c%ec%9a%94-overview" aria-label="3.1.1. 개요 (Overview)">3.1.1. 개요 (Overview)</a></li>
                <li>
                    <a href="#312-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98-architecture" aria-label="3.1.2. 아키텍처 (Architecture)">3.1.2. 아키텍처 (Architecture)</a></li>
                <li>
                    <a href="#313-%eb%b2%a4%ec%b9%98%eb%a7%88%ed%81%ac-benchmarks" aria-label="3.1.3. 벤치마크 (Benchmarks)">3.1.3. 벤치마크 (Benchmarks)</a></li>
                <li>
                    <a href="#314-%ed%95%99%ec%8a%b5-training" aria-label="3.1.4. 학습 (Training)">3.1.4. 학습 (Training)</a></li></ul>
                </li>
                <li>
                    <a href="#32-language-model-can-listen-while-speaking" aria-label="3.2. Language Model Can Listen While Speaking">3.2. Language Model Can Listen While Speaking</a><ul>
                        
                <li>
                    <a href="#321-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98-architecture" aria-label="3.2.1. 아키텍처 (Architecture)">3.2.1. 아키텍처 (Architecture)</a></li>
                <li>
                    <a href="#322-%eb%b2%a4%ec%b9%98%eb%a7%88%ed%81%ac-benchmarks" aria-label="3.2.2. 벤치마크 (Benchmarks)">3.2.2. 벤치마크 (Benchmarks)</a></li>
                <li>
                    <a href="#323-%ed%95%99%ec%8a%b5-training" aria-label="3.2.3. 학습 (Training)">3.2.3. 학습 (Training)</a></li></ul>
                </li>
                <li>
                    <a href="#33-personaplex-voice-and-role-control-for-full-duplex-conversational-speech-models" aria-label="3.3. PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models">3.3. PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models</a><ul>
                        
                <li>
                    <a href="#331-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98-architecture" aria-label="3.3.1. 아키텍처 (Architecture)">3.3.1. 아키텍처 (Architecture)</a></li>
                <li>
                    <a href="#332-%ed%95%99%ec%8a%b5-training" aria-label="3.3.2. 학습 (Training)">3.3.2. 학습 (Training)</a></li></ul>
                </li>
                <li>
                    <a href="#34-%ea%b8%b0%ed%83%80-%ea%b4%80%eb%a0%a8-%ec%97%b0%ea%b5%ac" aria-label="3.4. 기타 관련 연구">3.4. 기타 관련 연구</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Multi-Stream 아키텍처와 관련된 주요 연구들을 논문 단위로, 각각 개요, 아키텍처, 벤치마크, 학습 방식으로 나누어 상세히 분석합니다.</p>
<h3 id="31-shanks-simultaneous-hearing-and-thinking-for-spoken-language-models">3.1. Shanks: Simultaneous Hearing and Thinking for Spoken Language Models<a hidden class="anchor" aria-hidden="true" href="#31-shanks-simultaneous-hearing-and-thinking-for-spoken-language-models">#</a></h3>
<ul>
<li><strong>논문:</strong> &ldquo;Shanks: Simultaneous Hearing and Thinking for Spoken Language Models&rdquo; (arXiv 2024)</li>
<li><strong>핵심 의의:</strong> 사용자가 말하는 동안 모델이 동시에 &ldquo;내면의 추론(unspoken reasoning)&ldquo;을 생성하는 범용 추론 프레임워크를 제안했습니다. 이는 기존 모델들이 사용자의 발화가 끝난 후에야 처리를 시작하여 응답 지연이 발생하는 한계를 극복하고, 실시간 상호작용을 구현하기 위한 중요한 접근 방식입니다.</li>
</ul>
<h4 id="311-개요-overview">3.1.1. 개요 (Overview)<a hidden class="anchor" aria-hidden="true" href="#311-개요-overview">#</a></h4>
<p>Shanks는 스트리밍되는 음성 입력을 고정된 길이의 청크(chunk) 단위로 처리합니다. 각 음성 청크를 받을 때마다, 이전에 입력된 모든 음성과 내부 추론을 바탕으로 새로운 &ldquo;사고(thinking)&rdquo; 청크를 생성합니다. 이 내부적인 사고 과정을 통해 모델은 사용자의 말을 끊고 개입할지(interrupt) 또는 API 같은 도구를 호출할지 등을 사용자가 아직 말하는 도중에 결정할 수 있습니다. 이 프레임워크는 End-to-End SLM과 Cascade SLM(ASR+LLM) 두 가지 방식 모두에 적용 가능하도록 설계되었습니다.</p>
<h4 id="312-아키텍처-architecture">3.1.2. 아키텍처 (Architecture)<a hidden class="anchor" aria-hidden="true" href="#312-아키텍처-architecture">#</a></h4>
<p>Shanks는 사용자 음성을 고정된 시간 단위의 청크(S₁, S₂, &hellip;)로 분할하여 처리합니다. i번째 음성 청크(Sᵢ)를 입력받으면, 이를 바탕으로 i번째 사고 청크(Rᵢ)를 생성합니다. 즉, 사용자가 (i+1)번째 음성 청크를 말하는 동안, 모델은 i번째 사고 청크를 내부적으로 생성하며 &lsquo;듣는 것&rsquo;과 &lsquo;생각하는 것&rsquo;을 동시에 수행합니다. 이 과정에서 생성된 사고 청크(Rᵢ)는 발화되지 않으며, 모델의 내부적인 추론으로만 사용됩니다.</p>
<ol>
<li><strong>Shanks-E2E:</strong> End-to-End Spoken Language Model을 파인튜닝하여 &lsquo;들으면서 생각하는&rsquo; 능력을 학습시킵니다.</li>
<li><strong>Shanks-Cascade:</strong> ASR 모델과 강력한 텍스트 기반 LLM을 계단식으로 연결하여 더 강력한 추론 능력을 활용합니다.</li>
</ol>
<h4 id="313-벤치마크-benchmarks">3.1.3. 벤치마크 (Benchmarks)<a hidden class="anchor" aria-hidden="true" href="#313-벤치마크-benchmarks">#</a></h4>
<ol>
<li><strong>사용자 발화 중단 (수학 문제 풀이):</strong> Shanks-Cascade 모델은 78.3%의 유효한 중단률을 보여 가장 뛰어난 성능을 기록했습니다.</li>
<li><strong>청취 중 도구 사용 (목적 지향 대화):</strong> Shanks는 전체 API 호출의 56.9%를 사용자가 말을 마_치기 전에_ 성공적으로 수행했습니다.</li>
</ol>
<h4 id="314-학습-training">3.1.4. 학습 (Training)<a hidden class="anchor" aria-hidden="true" href="#314-학습-training">#</a></h4>
<p>GPT-4o를 사용하여 &lsquo;사고 청크&rsquo;와 중단/도구호출 시점을 포함한 학습 데이터를 생성하고, 표준 언어 모델링 손실 함수로 순차적으로 예측하도록 학습합니다.</p>
<h3 id="32-language-model-can-listen-while-speaking">3.2. Language Model Can Listen While Speaking<a hidden class="anchor" aria-hidden="true" href="#32-language-model-can-listen-while-speaking">#</a></h3>
<ul>
<li><strong>논문:</strong> &ldquo;Language Model Can Listen While Speaking&rdquo; (arXiv 2024)</li>
<li><strong>핵심 의의:</strong> 실시간 양방향 상호작용을 위해 &lsquo;말하면서 동시에 듣는&rsquo; 능력을 갖춘 End-to-End 모델(LSLM: Listening-while-Speaking Language Model)을 제안했습니다. 완전한 양방향 통신(Full Duplex Modeling)을 구현하여 실시간 중단(interruption)을 가능하게 합니다.</li>
</ul>
<h4 id="321-아키텍처-architecture">3.2.1. 아키텍처 (Architecture)<a hidden class="anchor" aria-hidden="true" href="#321-아키텍처-architecture">#</a></h4>
<p>LSLM은 다음 토큰을 예측하기 위해 <strong>말하기 채널</strong>(이전 생성 토큰)과 <strong>듣기 채널</strong>(실시간 음성 임베딩)의 정보를 융합(fusion)합니다. 트랜스포머 각 블록에서 정보를 결합하는 <strong>중간 융합(Middle Fusion)</strong> 방식이 가장 효과적이었습니다. 사용자의 중단이 감지되면, <code>[IRQ]</code>라는 특수 토큰을 통해 자신의 발화를 즉시 멈출 수 있습니다.</p>
<h4 id="322-벤치마크-benchmarks">3.2.2. 벤치마크 (Benchmarks)<a hidden class="anchor" aria-hidden="true" href="#322-벤치마크-benchmarks">#</a></h4>
<p>&lsquo;명령어 기반&rsquo; 및 &lsquo;음성 기반&rsquo; 중단 시나리오에서 평가되었습니다. 중간 융합 전략을 사용한 LSLM-MF 모델이 가장 좋은 성능을 보였으며, 노이즈 환경에서도 안정적인 중단 감지 능력을 보여주었습니다.</p>
<h4 id="323-학습-training">3.2.3. 학습 (Training)<a hidden class="anchor" aria-hidden="true" href="#323-학습-training">#</a></h4>
<p>TTS, 중단, 노이즈 데이터를 혼합하여 학습합니다. 50% 확률로 노이즈와 중단 상황을 무작위로 추가하여 강건성(robustness)을 높입니다.</p>
<h3 id="33-personaplex-voice-and-role-control-for-full-duplex-conversational-speech-models">3.3. PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models<a hidden class="anchor" aria-hidden="true" href="#33-personaplex-voice-and-role-control-for-full-duplex-conversational-speech-models">#</a></h3>
<ul>
<li><strong>논문:</strong> &ldquo;PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models&rdquo; (NVIDIA Research 2026)</li>
<li><strong>핵심 의의:</strong> 텍스트 프롬프트로 역할을 자유롭게 정의하고 다양한 음성으로 자연스러운 양방향 대화를 가능하게 하는 시스템입니다. &lsquo;동시에 듣고 말하는&rsquo; 이중 스트림(dual-stream) 구성을 통해 자연스러운 대화 리듬을 유지합니다.</li>
</ul>
<h4 id="331-아키텍처-architecture">3.3.1. 아키텍처 (Architecture)<a hidden class="anchor" aria-hidden="true" href="#331-아키텍처-architecture">#</a></h4>
<p><strong>Moshi</strong> 아키텍처(7B) 기반으로, 목소리 톤을 제어하는 <strong>음성 프롬프트</strong>와 역할/맥락을 정의하는 <strong>텍스트 프롬프트</strong>를 함께 사용하는 &lsquo;하이브리드 프롬프팅&rsquo;을 사용합니다. 이 이중 스트림 구성이 양방향 대화의 핵심입니다.</p>
<h4 id="332-학습-training">3.3.2. 학습 (Training)<a hidden class="anchor" aria-hidden="true" href="#332-학습-training">#</a></h4>
<p>자연스러운 대화 데이터 부족 문제를 해결하기 위해, <strong>실제 대화 데이터(Fisher corpus)와 합성 데이터를 혼합</strong>하여 사용합니다. LLM으로 실제 대화에 페르소나 설명을 생성하고, 합성 데이터는 스크립트와 음성을 모두 생성하여 사용합니다.</p>
<h3 id="34-기타-관련-연구">3.4. 기타 관련 연구<a hidden class="anchor" aria-hidden="true" href="#34-기타-관련-연구">#</a></h3>
<ul>
<li><strong>Front-Loading Reasoning (NVIDIA, 2025):</strong> LLM 학습 초반(사전-학습)에 추론 데이터를 포함하는 &ldquo;Front-Loading&quot;이 훨씬 효과적임을 증명했습니다. 사전-학습은 <strong>데이터의 다양성</strong>에, 후-학습(SFT)은 <strong>데이터의 품질</strong>에 더 민감하다는 &ldquo;비대칭 데이터 원칙&quot;을 제시했습니다. 이는 우리 아키텍처의 학습 전략 수립에 중요한 시사점을 줍니다.</li>
<li><strong>Multi-Token Attention (2025):</strong> 기존 어텐션이 단일 토큰 벡터에 의존하는 병목을 해결하기 위해, 컨볼루션(convolution) 연산으로 여러 토큰을 동시에 고려하는 **Multi-Token Attention (MTA)**을 제안했습니다. 이는 모델이 더 풍부한 컨텍스트 정보를 활용하도록 하여, 우리 아키텍처의 상위 계층(추상적 예측 모델) 성능을 높이는 데 기여할 수 있습니다.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://Finerestaurant.github.io/new-token/">The Polymath&#39;s Log</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
