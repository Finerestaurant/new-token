<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Archives on The Polymath&#39;s Log</title>
    <link>http://localhost:1313/new-token/archives/</link>
    <description>Recent content in Archives on The Polymath&#39;s Log</description>
    <generator>Hugo -- 0.156.0</generator>
    <language>ko-kr</language>
    <atom:link href="http://localhost:1313/new-token/archives/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CNN 영감 - Hubel &amp; Wiesel의 영향력</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/references/neuroscience/pmc_ncbi_cnn_inspiration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/references/neuroscience/pmc_ncbi_cnn_inspiration/</guid>
      <description>Robert H. Wurtz가 Hubel과 Wiesel의 시각 신경과학 분야의 기념비적인 연구 영향력을 다룬 논문 요약.</description>
    </item>
    <item>
      <title>Deep Representation Learning</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/references/criticism-of-existing-ai-paradigm/deep-representation-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/references/criticism-of-existing-ai-paradigm/deep-representation-learning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ma-lab-berkeley.github.io/deep-representation-learning-book/&#34;&gt;Learning Deep Representations of Data Distributions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;개요&#34;&gt;개요&lt;/h2&gt;
&lt;p&gt;이 장은 데이터 분포의 심층 표현 학습에 대한 비공식적인 소개를 제공한다. 지능의 진화 과정을 계통 발생적(phylogenetic), 개체 발생적(ontogenetic), 사회적(societal), 과학적(scientific) 지능으로 구분하여 설명하며, 생명체가 예측 가능한 환경에 적응하고 학습하는 방식을 조명한다. 또한, 1940년대 노버트 위너(Norbert Wiener)의 사이버네틱스 운동과 1950년대 인공지능(AI)의 등장을 통해 기계 지능의 기원을 탐구한다. 핵심 주제는 자연적이든 인공적이든 지능이 방대한 데이터에서 예측 가능한 정보를 학습하는 데 의존한다는 점이다. 데이터의 본질적인 특성인 &amp;ldquo;예측 가능성&amp;quot;과 &amp;ldquo;저차원성&amp;rdquo; 개념을 소개하며, 유용한 데이터가 고차원 공간 내의 저차원 표면에 존재한다는 점을 강조한다. 최근의 &amp;ldquo;AI 르네상스&amp;quot;는 인식, 생성, 예측과 같은 동물 수준 지능의 특성을 모방하는 데 중점을 두므로, 사이버네틱스의 목표에 더 가깝다고 주장한다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Does AI Have to Mimic the Human Brain</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/references/neuroscience/does_ai_mimic_human_brain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/references/neuroscience/does_ai_mimic_human_brain/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.2ndorderthinkers.com/p/does-ai-have-to-mimic-the-human-brain&#34;&gt;Does AI Have to Mimic the Human Brain&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;개요&#34;&gt;개요&lt;/h2&gt;
&lt;p&gt;이 글은 인공지능(AI) 연구가 인간의 뇌를 모방하는 데 지나치게 집착하는 경향에 의문을 제기합니다. 저자는 이러한 접근 방식이 AI의 잠재력을 제한할 수 있으며, 인간의 사고방식과는 전혀 다른 &amp;ldquo;새로운 종류의 지능&amp;quot;을 탐구해야 한다고 주장합니다. 비행 기술의 발전 과정(새 모방에서 공기역학 원리 이해로)을 예시로 들어, 직접적인 모방보다는 근본적인 원리 이해가 혁신으로 이어진다는 점을 강조합니다.&lt;/p&gt;
&lt;h2 id=&#34;핵심-키워드&#34;&gt;핵심 키워드&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;인공지능 (AI)&lt;/li&gt;
&lt;li&gt;인간 뇌 모방 (Mimicking human brain)&lt;/li&gt;
&lt;li&gt;이질적 지능 (Alien intelligence)&lt;/li&gt;
&lt;li&gt;새로운 종류의 지능 (New kind of intelligence)&lt;/li&gt;
&lt;li&gt;직접 모방 vs. 원리 이해 (Direct mimicry vs. understanding principles)&lt;/li&gt;
&lt;li&gt;오르니톱터 (Ornithopter)&lt;/li&gt;
&lt;li&gt;공기역학 (Aerodynamics)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;시사점&#34;&gt;시사점&lt;/h2&gt;
&lt;p&gt;현재 AI 개발이 인간의 인지 구조를 모방하는 데 집중되어 있지만, 이는 AI의 진정한 잠재력을 제한할 수 있습니다. 비행 기술의 사례처럼, AI 분야에서도 인간 뇌의 직접적인 모방을 넘어선 근본적으로 다른 형태의 지능을 개발하는 것이 혁신적인 돌파구를 마련할 수 있다는 시사점을 제공합니다. 이는 생물학적 모방에서 벗어나 새로운 계산 또는 지능 원리를 탐구하는 관점의 전환을 요구합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>H-Net</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/references/dynamic-chunking/h-nets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/references/dynamic-chunking/h-nets/</guid>
      <description>dynamic chunking을 활용하는 계층적 순환 모델 H-Net을 소개하는 논문입니다.</description>
    </item>
    <item>
      <title>Hibiki: High-Fidelity Simultaneous Speech-To-Speech Translation</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/references/dynamic-chunking/hibiki-simultaneous-translation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/references/dynamic-chunking/hibiki-simultaneous-translation/</guid>
      <description>This paper introduces Hibiki, a decoder-only model for simultaneous speech-to-speech and speech-to-text translation.</description>
    </item>
    <item>
      <title>Karpathy Attention Tweet</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/references/architecture/karpathy_attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/references/architecture/karpathy_attention/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://x.com/karpathy/status/1864023344435380613?lang=en&#34;&gt;Karpathy Attention Tweet&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;개요&#34;&gt;개요&lt;/h2&gt;
&lt;p&gt;Andrej Karpathy가 트랜스포머의 핵심 연산자인 &amp;lsquo;어텐션&amp;rsquo;의 기원에 대한 Dzmitry Bahdanau와의 이메일 내용을 공유한 트윗입니다. Karpathy는 어텐션이 2017년 &amp;ldquo;Attention is All You Need&amp;rdquo; 논문보다 3년 앞선 2014년 Bahdanau 등의 &amp;ldquo;Neural Machine Translation by Jointly Learning to Align and Translate&amp;rdquo; 논문에서 처음 소개되었음을 강조합니다. 이메일 내용은 &amp;lsquo;어텐션&amp;rsquo;이라는 이름이 어떻게 붙게 되었는지, 당시 비슷한 아이디어들이 어떻게 논의되었는지 등의 개발 비화를 담고 있습니다.&lt;/p&gt;
&lt;h2 id=&#34;핵심-키워드&#34;&gt;핵심 키워드&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;어텐션 (Attention)&lt;/li&gt;
&lt;li&gt;트랜스포머 (Transformer)&lt;/li&gt;
&lt;li&gt;Andrej Karpathy&lt;/li&gt;
&lt;li&gt;Dzmitry Bahdanau&lt;/li&gt;
&lt;li&gt;Attention is All You Need&lt;/li&gt;
&lt;li&gt;Neural Machine Translation&lt;/li&gt;
&lt;li&gt;가중 평균 (weighted average)&lt;/li&gt;
&lt;li&gt;RNNSearch&lt;/li&gt;
&lt;li&gt;Alex Graves&lt;/li&gt;
&lt;li&gt;Jason Weston&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;시사점&#34;&gt;시사점&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;획기적인 아이디어(&amp;ldquo;Attention&amp;rdquo;)가 처음 등장한 논문보다, 그 아이디어를 핵심으로 삼아 다른 요소들을 성공적으로 결합하고 단순화한 논문(&amp;ldquo;Attention is All You Need&amp;rdquo;)이 더 큰 주목을 받을 수 있음을 보여줍니다.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;어텐션&amp;rsquo;이라는 용어가 인간의 인지 과정(번역 시 원문 단어에 &amp;lsquo;주의&amp;rsquo;를 기울이는 것)에서 영감을 받아 이름 붙여졌다는 점을 알 수 있습니다.&lt;/li&gt;
&lt;li&gt;하나의 혁신이 여러 연구자들의 동시대적 고민과 아이디어 속에서 탄생하는 과학적 진보의 본질을 엿볼 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Multi-Stream 아키텍처: 개요, 비전 및 핵심 원리</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/multi-stream/01-vision-and-principles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/multi-stream/01-vision-and-principles/</guid>
      <description>&amp;#39;Stream Bundle&amp;#39; 아이디어에서 시작하여, Multi-Stream 아키텍처의 비전과 핵심 원리를 다룹니다.</description>
    </item>
    <item>
      <title>Multi-Stream: 관련 연구 심층 분석</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/multi-stream/03-related-research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/multi-stream/03-related-research/</guid>
      <description>Multi-Stream 아키텍처와 관련된 주요 연구(Shanks, LSLM, PersonaPlex 등)들을 논문 단위로 심층 분석합니다.</description>
    </item>
    <item>
      <title>Multi-Stream: 분석, 목표 및 결론</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/multi-stream/04-conclusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/multi-stream/04-conclusion/</guid>
      <description>Multi-Stream 아키텍처의 잠재력과 한계를 분석하고, 최종 목표와 결론을 제시합니다.</description>
    </item>
    <item>
      <title>Multi-Stream: 아키텍처 설계</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/multi-stream/02-architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/multi-stream/02-architecture/</guid>
      <description>계층적 예측 모델의 개념을 구체적인 Multi-Stream 아키텍처 설계로 전환하고, 각 구성 요소의 역할을 정의합니다.</description>
    </item>
    <item>
      <title>Physical AI (Embodied AI) - From Brain in a Vat to Reality</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/physical-ai/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/physical-ai/</guid>
      <description>LLM과 같은 &amp;#39;통 속의 뇌&amp;#39;가 현실 세계의 &amp;#39;신체&amp;#39;를 가질 때 발생하는 근본적인 문제들을 고찰합니다.</description>
    </item>
    <item>
      <title>Starlight Markdown 문법 가이드</title>
      <link>http://localhost:1313/new-token/archives/agents/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/agents/</guid>
      <description>Starlight 프로젝트에서 .md 파일을 작성할 때 지켜야 할 주요 문법 및 Frontmatter 가이드입니다.</description>
    </item>
    <item>
      <title>Superposition: 신경망의 중첩과 지식의 조작</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/superposition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/superposition/</guid>
      <description>Anthropic의 &amp;#39;Toy Models of Superposition&amp;#39; 연구를 바탕으로, 신경망의 내부 표현 중첩 현상을 분석하고 Multi-Stream 아키텍처에서의 Unlearning 및 지식 조작 가능성을 탐구합니다.</description>
    </item>
    <item>
      <title>What I cannot create, I do not understand</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/references/agents/what_i_cannot_create_i_do_not_understand/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/references/agents/what_i_cannot_create_i_do_not_understand/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://journals.biologists.com/jcs/article/130/18/2941/56386/What-I-cannot-create-I-do-not-understand&#34;&gt;What I cannot create, I do not understand&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;개요&#34;&gt;개요&lt;/h2&gt;
&lt;p&gt;이 사설은 리처드 파인만의 &amp;ldquo;내가 만들 수 없는 것은 이해하지 못한다&amp;quot;는 인용구에서 영감을 받아 세포 생물학의 &amp;lsquo;상향식 재구성&amp;rsquo; 접근 방식의 중요성을 강조하며, &amp;lsquo;세포 생물학 재구성&amp;rsquo;에 대한 특별호 발행을 알립니다. 세포의 구성 요소를 이해하는 데 큰 진전이 있었지만, 세포 구성 요소들이 더 높은 형태와 기능으로 통합되는 방식에 대한 완전한 분자적, 물리적 이해는 여전히 부족함을 지적합니다.&lt;/p&gt;
&lt;h2 id=&#34;핵심-키워드&#34;&gt;핵심 키워드&lt;/h2&gt;
&lt;p&gt;세포 생물학 재구성, 상향식 접근 방식, 리처드 파인만, 분자 및 기계적 통찰력, 세포 조직, 세포 기능, 미세 패턴화, 미세 유체학.&lt;/p&gt;</description>
    </item>
    <item>
      <title>기계적 해석학 (Mechanistic Interpretability)</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/mechanistic-interpretability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/mechanistic-interpretability/</guid>
      <description>Chris Olah 등을 중심으로 한 기계적 해석학의 학계 내 위치와 그 의의를 분석합니다.</description>
    </item>
    <item>
      <title>기존 LLM의 구조 및 토큰 학습 방식 분석</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/experimental-design/current_status/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/experimental-design/current_status/</guid>
      <description>기존 대규모 언어 모델(LLM)의 기본 구조와 토큰 학습 메커니즘을 분석합니다.</description>
    </item>
    <item>
      <title>동적 시스템 구현을 위한 연구 로드맵</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/dynamic-adaptive-systems/1_research_plan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/dynamic-adaptive-systems/1_research_plan/</guid>
      <description>&lt;p&gt;인간 지능은 &lt;strong&gt;유한한 계산 자원&lt;/strong&gt;이라는 제약 속에서 &lt;strong&gt;방대한 정보량&lt;/strong&gt;을 효과적으로 처리하여 &lt;strong&gt;정확하고 장기적인 예측&lt;/strong&gt;을 수행하도록 진화해왔습니다. 이러한 능력을 달성하기 위해 인간 지능은 다음과 같은 핵심 역량들을 발달시켰습니다.&lt;/p&gt;
&lt;h3 id=&#34;핵심-능력&#34;&gt;핵심 능력&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;압축 (Compression)&lt;/strong&gt;: 유한한 자원 제약 하에서 정보의 본질을 추출하고 효율적으로 표현하는 능력입니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;계층 (Hierarchy)&lt;/strong&gt;: 정보를 여러 수준으로 구조화하고 조직화하여 복잡성을 관리하는 능력입니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;능동성 (Proactivity)&lt;/strong&gt;: 정보를 받아들이는 것에 그치지 않고, 능동적으로 정보를 탐색하고 처리하며, 다음 단계를 선제적으로 결정하는 능력입니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;유연성 (Flexibility)&lt;/strong&gt;: 상황 변화에 따라 기존의 처리 방식이나 필터를 조정하고 새로운 방식으로 적응하는 능력입니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;창조성 (Creativity)&lt;/strong&gt;: 새로운 필터, 지식, 또는 해결책을 생성하는 능력입니다.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;episodic-prediction-및-dynamic-chunking&#34;&gt;Episodic Prediction 및 dynamic chunking&lt;/h3&gt;
&lt;p&gt;이러한 인간 지능의 능력들은 &lt;strong&gt;Episodic prediction&lt;/strong&gt; 및 &lt;strong&gt;dynamic chunking&lt;/strong&gt;과 같은 개념들과 깊이 연관됩니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>아이디어 개선 및 구현 방향성</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/dynamic-adaptive-systems/2_implementation_direction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/dynamic-adaptive-systems/2_implementation_direction/</guid>
      <description>동적 시스템 구현을 위한 아이디어 개선과 구체적인 구현 방향성을 다룹니다.</description>
    </item>
    <item>
      <title>연구에 앞서서...</title>
      <link>http://localhost:1313/new-token/archives/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/introduction/</guid>
      <description>이 문서에서는 연구의 introduction을 다룹니다.</description>
    </item>
    <item>
      <title>유한한 계산 자원에서의 효율적인 압축</title>
      <link>http://localhost:1313/new-token/archives/academic-landscape/references/compression/efficient_compression_finite_resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/academic-landscape/references/compression/efficient_compression_finite_resources/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pmc.ncbi.nlm.nih.gov/articles/PMC9434449/&#34;&gt;Prediction in the Aging Brain: Merging Cognitive, Neurological, and Evolutionary Perspectives&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;개요&#34;&gt;개요&lt;/h2&gt;
&lt;p&gt;이 논문은 노화하는 뇌의 예측 능력에 대한 인지적, 신경학적, 진화론적 관점을 통합하여 분석합니다. 연구는 노화 뇌가 새로운 정보를 학습하는 능력보다는 기존 지식을 활용하여 예측하는 능력이 더욱 효과적으로 변화한다는 가설을 제시합니다. 이는 노화를 단순히 인지 기능의 저하로만 보는 관점에서 벗어나, 일부 인지 기능이 안정되거나 향상될 수 있음을 시사하며, 이러한 &amp;lsquo;학습-예측 전환&amp;rsquo; 현상에 대한 진화론적 모델, 노화 및 가소성 원리에 기반한 잠재적 설명을 제공합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>지능의 본질에 관한 사유의 틀 (Drafting Table)</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/intelligence-framework/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/intelligence-framework/</guid>
      <description>&lt;h1 id=&#34;지능의-본질에-관한-사유의-틀-drafting-table&#34;&gt;지능의 본질에 관한 사유의 틀 (Drafting Table)&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;이 문서는 AI와 지능의 본질에 대한 나의 공리(Axioms)를 세워나가는 살아있는 기록입니다. 학계의 난제들에 답하며 나만의 지능 이론을 조각해 나갑니다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-지능의-공리-my-axioms&#34;&gt;1. 지능의 공리 (My Axioms)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;생존을 위한 예측 (Prediction for Survival):&lt;/strong&gt; 지능의 가장 자명한 목적은 생존이며, 이를 위해 뇌는 더 넓은 범위의 미래를 정확하게 예측하도록 진화했다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;에너지 효율성 원칙 (Principle of Energy Efficiency):&lt;/strong&gt; 뇌는 새로운 회로를 생성/삭제하는 것보다, 기존 회로를 재사용하는 것을 선호한다. 이는 새로운 환경 적응을 어렵게 만드는 원인이기도 하다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;이산적 처리와 계층적 위임 (Discrete Processing &amp;amp; Hierarchical Delegation):&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;인간의 뇌는 연속적인 세계를 &amp;lsquo;이산적인 단위(Symbol/Concept/Keyframe)&amp;lsquo;로 처리한다.&lt;/li&gt;
&lt;li&gt;상위 계층(뇌)은 이산적인 목표만 설정하고, 하위 계층(척수/신체)이 이를 연속적인 움직임으로 변환(Conversion)한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-핵심-난제에-대한-응답-the-dialectic&#34;&gt;2. 핵심 난제에 대한 응답 (The Dialectic)&lt;/h2&gt;
&lt;h3 id=&#34;q1-모라벡의-역설-moravecs-paradox와-신체성&#34;&gt;Q1. 모라벡의 역설 (Moravec&amp;rsquo;s Paradox)와 신체성&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;질문:&lt;/strong&gt; 지능에 &amp;lsquo;신체(Body)&amp;lsquo;는 필수적인가, 아니면 부산물인가? 왜 아이에게 쉬운 걷기가 AI에게는 어려운가?&lt;/p&gt;</description>
    </item>
    <item>
      <title>진행과정</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/progress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/progress/</guid>
      <description>&lt;p&gt;컨텐츠가 곧 추가될 예정입니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>초기 가설 수립</title>
      <link>http://localhost:1313/new-token/archives/my-thoughts/hypothesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/new-token/archives/my-thoughts/hypothesis/</guid>
      <description>&lt;details&gt;
&lt;summary&gt;사용자 질문&lt;/summary&gt;
&lt;p&gt;인간의 예측은 토큰 하나가 아니고, 다른 단위를 사용하는 거 같아. 지금부터 내 머릿속에 있는 이미지를 설명해줄게.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;인간의 예측은 계층 관계를 이루는 상황을 단위로 삼아. 눈앞에 바로 있는 일은 굉장히 구체적으로 예측하고, 저 멀리 있는 일은 어렴풋이 예측하는거 같아. 하지만, 구체적으로 예측하는 만큼, 더 변화무쌍하는 것을 허락하고, 멀리있는 일일수록 더 강고해. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;조금 더 예시를 들어보자면 이런것이야. 난 지금 카페에서 커피를 마시면서 lm과 대화를 하고 있는 지금 이 사건을 스트레스 없이 받아들이고 있는 이유는, 이미 선입견 - 선제적으로 입력되있는 관점이 있는거야. 그 관점들은 계층적으로 이루어져있어. 상위계층으로 가면, 한국사회에 대한 월드모델 시뮬레이션, 그것보다 하위로 가면 서울시의 분위기 시뮬레이션, 그 아래로 가면, 이 카페의 시뮬레이션, 이 카페에서 일어날수 있는 시나리오를 머릿속으로 이미 그리고 있는 거지. 그 관점이 위로가면 위로갈수록 더 무겁고, 바뀌기 어려운 것들이야. 왜냐하면 이 계층관계는 상위가 하위를 품고 있고, 이 관계에 의해 상위로 가면 갈수록 정보량이 많아지기 때문이야. &lt;/p&gt;</description>
    </item>
  </channel>
</rss>
